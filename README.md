ğŸŒ Intelligent RL-Based Routing System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-FF4B4B.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **M.Tech Computer Science Project**  
> Madan Mohan Malaviya University of Technology, Gorakhpur

An intelligent packet routing system that uses **Reinforcement Learning** (Q-Learning & Deep Q-Network) with **Transfer Learning** to optimize network routing. Compares RL approaches with traditional algorithms (Dijkstra & Bellman-Ford).

![System Demo](docs/images/demo.gif)

---

## âœ¨ Key Features

- ğŸ§  **Dual RL Algorithms**: Q-Learning and Deep Q-Network (DQN)
- ğŸ”„ **Transfer Learning**: Pre-trained models work across network topologies
- ğŸ“Š **Multiple Topologies**: Mesh, Ring, Tree, Random, and Custom networks
- ğŸ¨ **Interactive UI**: Built with Streamlit for easy experimentation
- ğŸ“ˆ **Comprehensive Metrics**: Path cost, latency, energy, hop count
- ğŸ” **Algorithm Comparison**: RL vs Traditional routing
- ğŸ’¾ **Model Persistence**: Save and load trained models
- ğŸ“‰ **Performance Visualization**: Real-time training and comparison charts

---

## ğŸ“‘ Table of Contents

- [Features](#-key-features)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Usage](#-usage)
- [Algorithms](#-algorithms)
- [Performance](#-performance)
- [Screenshots](#-screenshots)
- [Research](#-research)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Git (for cloning)

### Clone Repository

```bash
git clone https://github.com/shivanshpandeygp/intelligent_routing_system.git
cd intelligent_routing_system
```

### Install Dependencies

#### Using requirements.txt

```bash
pip install -r requirements.txt
```

---

## âš¡ Quick Start

### 1. Run the Application

```bash
streamlit run frontend/app.py
```

Or on Windows:

```bash
run_project.bat
```

### 2. Open in Browser

Navigate to: [**http://localhost:8501**](http://localhost:8501)

### 3. Create a Network

1. Go to **"ğŸ“Š Network Setup"** tab
2. Choose topology (Mesh/Ring/Tree/Random)
3. Set number of nodes (5-20)
4. Click **"ğŸ”¨ Create Network"**

### 4. Train Models

1. Go to **"ğŸ¤– Train Models"** tab
2. Select source and destination nodes
3. Click **"Train Q-Learning"** or **"Train DQN"**
4. Wait for training to complete

### 5. Discover Routes

1. Go to **"ğŸ” Route Discovery"** tab
2. Select algorithms to compare
3. View paths, costs, and visualizations

---

## ğŸ“ Project Structure

```plaintext
intelligent_routing_system/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ graph_manager.py       # Network graph management
â”‚   â”œâ”€â”€ traditional_routing.py # Dijkstra & Bellman-Ford
â”‚   â”œâ”€â”€ q_learning_routing.py  # Q-Learning implementation
â”‚   â”œâ”€â”€ dqn_routing.py         # Deep Q-Network implementation
â”‚   â”œâ”€â”€ reward_designer.py     # Reward function design
â”‚   â””â”€â”€ performance_metrics.py # Metric calculations
â”‚
â”œâ”€â”€ frontend/
|   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                 # Main Streamlit application
â”‚   â”œâ”€â”€ visualizer.py          # Network visualization
â”‚   â””â”€â”€ dashboard.py           # Performance dashboard
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark_runner.py
â”‚   â””â”€â”€ topology_generator.py
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pretrained/            # Global pre-trained models
â”‚   â””â”€â”€ network_specific/      # Network-specific models
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ images/                # Documentation images
â”‚       â”œâ”€â”€ dashboard.png
â”‚       â”œâ”€â”€ network_setup.png
â”‚       â”œâ”€â”€ network_setup1.png
|       â”œâ”€â”€ route_discovery.png
â”‚       â””â”€â”€ training.png
|
â”œâ”€â”€ test/
|   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_dqn.py
â”‚   â”œâ”€â”€ test_graph_manager.py
|   â”œâ”€â”€ test_q_learning.py
â”‚   â””â”€â”€ test_traditional.py
|
â”œâ”€â”€ __init__.py
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ CONTRIBUTING.md            # Contribution guidelines
â”œâ”€â”€ CODE_OF_CONDUCT.md         # code of conduct
â””â”€â”€ run_project.bat            # run the project
```

---

## ğŸ’¡ Usage

### Creating Custom Networks

```python
from backend.graph_manager import NetworkGraph

# Create a custom network
network = NetworkGraph.create_custom(
    num_nodes=10,
    edges=[
        (0, 1, 10.0),  # (source, dest, weight)
        (1, 2, 15.0),
        (2, 3, 12.0),
    ]
)
```

### Training Models Programmatically

```python
from backend.q_learning_routing import QLearningRouting
from backend.dqn_routing import DQNRouting
from backend.config import Config

config = Config()
config.QL_EPISODES = 1000

# Train Q-Learning
ql_agent = QLearningRouting(network, config)
ql_agent.train(source=0, destination=9)

# Train DQN
dqn_agent = DQNRouting(network, config)
dqn_agent.train(source=0, destination=9)
```

### Finding Routes

```python
# Q-Learning routing
path, cost, time = ql_agent.find_path(0, 9)
print(f"Q-Learning: {path}, Cost: {cost:.2f}")

# DQN routing
path, cost, time = dqn_agent.find_path(0, 9)
print(f"DQN: {path}, Cost: {cost:.2f}")
```

---

## ğŸ§  Algorithms

### Traditional Algorithms

1. **Dijkstra's Algorithm**

   - Optimal shortest path
   - Time Complexity: O((V + E) log V)
   - Use case: Static networks with non-negative weights

2. **Bellman-Ford Algorithm**
   - Handles negative weights
   - Time Complexity: O(VE)
   - Use case: Networks with negative edge weights

### Reinforcement Learning Algorithms

3. **Q-Learning**

   - Model-free tabular RL
   - Learns state-action Q-values
   - Use case: Small to medium networks
   - Advantages: Fast convergence, interpretable

4. **Deep Q-Network (DQN)**
   - Neural network Q-value approximation
   - Better generalization
   - Use case: Large, complex networks
   - Advantages: Scalable, transfer learning

### Transfer Learning

- **Global Models**: Pre-trained on multiple networks
- **Network-Specific Models**: Fine-tuned for specific topologies
- **Zero-Shot Routing**: Use pre-trained models without training
- **Continual Learning**: Accumulates knowledge over time

---

## ğŸ“Š Performance

### Metrics Evaluated

| Metric                 | Description                  | Unit         |
| ---------------------- | ---------------------------- | ------------ |
| **Path Cost**          | Sum of edge weights          | Numeric      |
| **Computation Time**   | Algorithm execution time     | Milliseconds |
| **Latency**            | End-to-end delay             | Milliseconds |
| **Energy Consumption** | Power usage estimate         | Energy units |
| **Hop Count**          | Number of intermediate nodes | Integer      |
| **Success Rate**       | Route discovery success      | Percentage   |

### Experimental Results

**10-Node Mesh Network (Source: 0, Destination: 9)**

| Algorithm    | Path Cost | Time (ms) | Success Rate |
| ------------ | --------- | --------- | ------------ |
| Dijkstra     | 18.0      | 0.01      | 100%         |
| Bellman-Ford | 18.0      | 0.02      | 100%         |
| Q-Learning   | 18.5      | 0.15      | 95%          |
| DQN          | 20.2      | 2.50      | 85%          |

---

## ğŸ“¸ Screenshots

### Network Setup

![Network Setup](docs/images/network_setup.png)
![Network Setup](docs/images/network_setup1.png)

### Training Interface

![Training](docs/images/training.png)

### Route Discovery

![Route Discovery](docs/images/route_discovery.png)

### Performance Dashboard

![Dashboard](docs/images/dashboard.png)

---

## ğŸ”¬ Research

### Problem Statement

Traditional routing algorithms (Dijkstra, Bellman-Ford) are optimal for static networks but struggle with:

- Dynamic topology changes
- Multi-objective optimization
- Real-time adaptation
- Learning from experience

### Research Questions

1. Can RL-based routing match traditional algorithms in static scenarios?
2. How does transfer learning improve routing across different topologies?
3. What are the trade-offs between Q-Learning and DQN in routing?

### Key Contributions

- Comparative analysis of RL vs traditional routing
- Transfer learning framework for routing algorithms
- Multi-metric performance evaluation
- Open-source implementation for reproducibility

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ› Known Issues

- Q-Learning may fail on sparse networks without sufficient training
- DQN requires more episodes for convergence than Q-Learning
- Network visualization slow for >50 nodes

See [Issues](https://github.com/shivanshpandeygp/intelligent_routing_system/issues) for more.

---

## ğŸ”® Future Work

- [ ] Add more RL algorithms (A3C, PPO, SAC)
- [ ] Real network trace integration
- [ ] Dynamic topology change simulation
- [ ] Mobile network topology support
- [ ] REST API for external integration
- [ ] Docker containerization

---

## ğŸ“š References

1. Sutton, R. S., & Barto, A. G. (2018). _Reinforcement Learning: An Introduction_. MIT Press.
2. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. _Nature_, 518(7540), 529-533.
3. Tanenbaum, A. S., & Wetherall, D. J. (2011). _Computer Networks_. Prentice Hall.

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ“ Academic Use

If you use this project in academic research, please cite:

---

## ğŸ“§ Contact

**Shivansh Pandey**  
M.Tech Computer Science  
Madan Mohan Malaviya University of Technology, Gorakhpur

ğŸ“§ Email: sdutt081@gmail.com

**Project Link:** [https://github.com/shivanshpandeygp/intelligent_routing_system](https://github.com/shivanshpandeygp/intelligent_routing_system)

---

## ğŸ™ Acknowledgments

- **MMMUT Gorakhpur** for academic support
- **Project Guide:** [Dr. Vimal Kumar], Department of Computer Science
- **PyTorch** and **Streamlit** communities
- **NetworkX** library developers
- All open-source contributors

---

<div align="center">

**Made with â¤ï¸ for academic research and open-source community**

If you find this project helpful, please â­ star the repository!

</div>
